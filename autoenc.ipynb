{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\nimport keras\nimport numpy as np\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-06-03T10:13:41.615907Z","iopub.execute_input":"2022-06-03T10:13:41.616374Z","iopub.status.idle":"2022-06-03T10:13:49.274904Z","shell.execute_reply.started":"2022-06-03T10:13:41.616287Z","shell.execute_reply":"2022-06-03T10:13:49.273829Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# loading mnist dataset\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n# normalising and reshaping the data\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (x_train.shape[0], 28, 28, 1))\nx_test = np.reshape(x_test, (x_test.shape[0], 28, 28, 1))\nx_train.shape, x_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-03T10:17:24.093527Z","iopub.execute_input":"2022-06-03T10:17:24.094401Z","iopub.status.idle":"2022-06-03T10:17:24.854494Z","shell.execute_reply.started":"2022-06-03T10:17:24.094359Z","shell.execute_reply":"2022-06-03T10:17:24.853349Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Building the Autoencoder\n**Autoencoder is made up of two parts â€“ encoder and decoder. The architecture of the encoder and decoder are mirror images of one another. For example, the encoder has max-pooling layers to reduce the dimension of the features while the decoder has upsampling layers that increase the number of features.**","metadata":{}},{"cell_type":"code","source":"input_img = keras.Input(shape=(28, 28, 1))\n\nx = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n\n# the shape is 4,4,8 here\n\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(32, (3, 3), activation='relu')(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = keras.Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nautoencoder.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T10:29:12.764190Z","iopub.execute_input":"2022-06-03T10:29:12.764665Z","iopub.status.idle":"2022-06-03T10:29:13.148479Z","shell.execute_reply.started":"2022-06-03T10:29:12.764631Z","shell.execute_reply":"2022-06-03T10:29:13.147362Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training the Autoencoder","metadata":{}},{"cell_type":"code","source":"autoencoder.fit(x_train, x_train, epochs=100, batch_size=128, validation_data=(x_test, x_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T10:29:58.216720Z","iopub.execute_input":"2022-06-03T10:29:58.217106Z","iopub.status.idle":"2022-06-03T11:20:21.550528Z","shell.execute_reply.started":"2022-06-03T10:29:58.217077Z","shell.execute_reply":"2022-06-03T11:20:21.549363Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Encoder and Decoder\n**The first 7 layers represent the encoder while the remaining layers represent the decoder. We can extract the respective layers from the trained autoencoder and build the encoder and decoder.**","metadata":{}},{"cell_type":"code","source":"encoder = Sequential()\ndecoder = Sequential()\nfor layer in autoencoder.layers[:8]: encoder.add(layer)\nfor layer in autoencoder.layers[8:]: decoder.add(layer)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:22:27.905963Z","iopub.execute_input":"2022-06-03T11:22:27.906372Z","iopub.status.idle":"2022-06-03T11:22:27.946434Z","shell.execute_reply.started":"2022-06-03T11:22:27.906339Z","shell.execute_reply":"2022-06-03T11:22:27.945728Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Testing the Autoencoder\n**These are the first ten samples from the training set.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nn = 10\nl = []\nfor i in range(n):\n    plt.subplot(2,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.xlabel(y_train[i])\n    l.append(i)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:24:33.458696Z","iopub.execute_input":"2022-06-03T11:24:33.459614Z","iopub.status.idle":"2022-06-03T11:24:33.975853Z","shell.execute_reply.started":"2022-06-03T11:24:33.459539Z","shell.execute_reply":"2022-06-03T11:24:33.974824Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Encode the sample input images into 128-feature encodings using the encoder. Then, we will use the decoder to regenerate the input images from the 128-feature encodings created by the encoder.**","metadata":{}},{"cell_type":"code","source":"encoded_sample = encoder.predict(x_train[0:10]) # encoding\nencoded_sample.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:24:38.582241Z","iopub.execute_input":"2022-06-03T11:24:38.582671Z","iopub.status.idle":"2022-06-03T11:24:38.744824Z","shell.execute_reply.started":"2022-06-03T11:24:38.582623Z","shell.execute_reply":"2022-06-03T11:24:38.743893Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"decoded_sample = decoder.predict(encoded_sample) # decoding\ndecoded_sample.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:24:41.151166Z","iopub.execute_input":"2022-06-03T11:24:41.152148Z","iopub.status.idle":"2022-06-03T11:24:41.303895Z","shell.execute_reply.started":"2022-06-03T11:24:41.152111Z","shell.execute_reply":"2022-06-03T11:24:41.302866Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**These are the generated images by the decoder using the 128-feature encodings from the encoder.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nn = 10\nfor i in range(n):\n    plt.subplot(2,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(decoded_sample[i], cmap='gray')\n    plt.xlabel(y_train[i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:24:53.055394Z","iopub.execute_input":"2022-06-03T11:24:53.055810Z","iopub.status.idle":"2022-06-03T11:24:53.803666Z","shell.execute_reply.started":"2022-06-03T11:24:53.055777Z","shell.execute_reply":"2022-06-03T11:24:53.802551Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Now, let us try to generate a new set of images. Essentially, variational autoencoders need to be used for this purpose. Autoencoders can be used for generating new images but the drawback is that they might produce a lot of noise if the encodings are too different and non-overlapping.**","metadata":{}},{"cell_type":"code","source":"starting, ending = encoder.predict(x_train[0:2])\n# Interpolating new encodings\nvalues = np.linspace(starting, ending, 10)\ngenerated_images = decoder.predict(values) # Generate new images\nplt.figure(figsize=(10,5))\nn = 10\nfor i in range(n):\n    plt.subplot(2,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(generated_images[i], cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T11:24:57.694405Z","iopub.execute_input":"2022-06-03T11:24:57.695478Z","iopub.status.idle":"2022-06-03T11:24:58.489959Z","shell.execute_reply.started":"2022-06-03T11:24:57.695404Z","shell.execute_reply":"2022-06-03T11:24:58.488496Z"},"trusted":true},"execution_count":15,"outputs":[]}]}